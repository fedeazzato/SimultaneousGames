{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1 - Simultaneous Games\n",
    "\n",
    "Alumnos:\n",
    "José Ignacio Cáceres\n",
    "Federico Azzato\n",
    "\n",
    "\n",
    "Para la tarea se implementaron agentes inteligentes que aplican las técnicas de Fictitious Play y Regret Matching vistas en el curso, y se estudió el comportamiento de estos agentes interactuando entre sí, y junto a un agente random, en distintos juegos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base.agent import Agent\n",
    "from base.game import SimultaneousGame\n",
    "from typing import Callable\n",
    "from typing import Iterable\n",
    "import numpy as np\n",
    "\n",
    "def EvaluateGame(game_name:str, game:SimultaneousGame, progress_display:Callable[[SimultaneousGame, Agent],None], agent_classes:Iterable[tuple[type[Agent],str]], iterations:int=10000):\n",
    "    print('')\n",
    "    print(f\"Evaluating agents { ' vs '.join([agent_name for _, agent_name in agent_classes]) } in {game_name} game for {iterations} iterations\")\n",
    "    agents = {}\n",
    "    game.reset()\n",
    "    for i, agent in enumerate(game.agents):\n",
    "        agents[agent] = agent_classes[i][0](game=game, agent=agent)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        actions = dict(map(lambda agent: (agent, agents[agent].action()), game.agents))\n",
    "        game.step(actions)\n",
    "        if (i+1)%(iterations/10) == 0:\n",
    "            print(f\"Progress: {i+1}/{iterations} - \",\n",
    "                dict(map(lambda agent: (agent, progress_display(game, agents[agent])), game.agents)))\n",
    "\n",
    "    print(\"Final policy: \", dict(map(lambda agent: (agent, agents[agent].policy()), game.agents)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.fictitiousplay import FictitiousPlay\n",
    "from agents.regretmatching import RegretMatching\n",
    "from agents.random_agent import RandomAgent\n",
    "\n",
    "def EvaluateAll(game_name:str, game:SimultaneousGame, progress_display:Callable[[SimultaneousGame, Agent],None]=None, iterations:int=10000):\n",
    "    smart_agent_classes = [\n",
    "        (FictitiousPlay, 'FP'),\n",
    "        (RegretMatching, 'RM')\n",
    "        ]\n",
    "    agent_combinations = []\n",
    "\n",
    "    if progress_display is None:\n",
    "        progress_display = lambda _, agent: agent.policy()\n",
    "\n",
    "    for i in range(len(smart_agent_classes)):\n",
    "        agent_combinations.append([smart_agent_classes[i], (RandomAgent, 'RA')])\n",
    "        for j in range(i+1):\n",
    "            agent_combinations.append([smart_agent_classes[i], smart_agent_classes[j]])\n",
    "\n",
    "    print(f\"Started Evaluation of {game_name} game\")\n",
    "\n",
    "    for agent_classes in agent_combinations:\n",
    "        EvaluateGame(game_name, game, progress_display, agent_classes, iterations)\n",
    "\n",
    "    print(f\"Completed Evaluation of {game_name} game\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Pennies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Evaluation of Matching Pennies game\n",
      "\n",
      "Evaluating agents FP vs RA in Matching Pennies game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.98058252, 0.01941748]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.93103448, 0.06896552]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.95379538, 0.04620462]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.96526055, 0.03473945]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.972167, 0.027833]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.97678275, 0.02321725]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.89544808, 0.10455192]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.87048568, 0.12951432]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.88150609, 0.11849391]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.89332004, 0.10667996]), 'agent_1': array([0.5, 0.5])}\n",
      "Final policy:  {'agent_0': array([0.89332004, 0.10667996]), 'agent_1': array([0.5, 0.5])}\n",
      "\n",
      "\n",
      "Evaluating agents FP vs FP in Matching Pennies game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.44660194, 0.55339806]), 'agent_1': array([0.50717703, 0.49282297])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.4679803, 0.5320197]), 'agent_1': array([0.50855746, 0.49144254])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.51155116, 0.48844884]), 'agent_1': array([0.48604269, 0.51395731])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.47766749, 0.52233251]), 'agent_1': array([0.49567367, 0.50432633])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.48707753, 0.51292247]), 'agent_1': array([0.51139742, 0.48860258])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.4933665, 0.5066335]), 'agent_1': array([0.5153019, 0.4846981])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.49217639, 0.50782361]), 'agent_1': array([0.51242016, 0.48757984])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.48630137, 0.51369863]), 'agent_1': array([0.50528278, 0.49471722])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.48726467, 0.51273533]), 'agent_1': array([0.49530127, 0.50469873])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.5, 0.5]), 'agent_1': array([0.48531608, 0.51468392])}\n",
      "Final policy:  {'agent_0': array([0.5, 0.5]), 'agent_1': array([0.48531608, 0.51468392])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RA in Matching Pennies game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.50215642, 0.49784358]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.50333652, 0.49666348]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.50701579, 0.49298421]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.50927067, 0.49072933]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.50967011, 0.49032989]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.50903052, 0.49096948]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.50795547, 0.49204453]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.50731353, 0.49268647]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.50733151, 0.49266849]), 'agent_1': array([0.5, 0.5])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.50812793, 0.49187207]), 'agent_1': array([0.5, 0.5])}\n",
      "Final policy:  {'agent_0': array([0.50812793, 0.49187207]), 'agent_1': array([0.5, 0.5])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs FP in Matching Pennies game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.56832941, 0.43167059]), 'agent_1': array([0.63461538, 0.36538462])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.52603975, 0.47396025]), 'agent_1': array([0.42892157, 0.57107843])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.49887678, 0.50112322]), 'agent_1': array([0.44243421, 0.55756579])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.48470821, 0.51529179]), 'agent_1': array([0.49628713, 0.50371287])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.49817204, 0.50182796]), 'agent_1': array([0.54464286, 0.45535714])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.49730221, 0.50269779]), 'agent_1': array([0.4544702, 0.5455298])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.49228617, 0.50771383]), 'agent_1': array([0.4765625, 0.5234375])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.49312437, 0.50687563]), 'agent_1': array([0.48445274, 0.51554726])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.49249083, 0.50750917]), 'agent_1': array([0.48119469, 0.51880531])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.48837348, 0.51162652]), 'agent_1': array([0.43326693, 0.56673307])}\n",
      "Final policy:  {'agent_0': array([0.48837348, 0.51162652]), 'agent_1': array([0.43326693, 0.56673307])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RM in Matching Pennies game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.49352176, 0.50647824]), 'agent_1': array([0.51159126, 0.48840874])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.49607079, 0.50392921]), 'agent_1': array([0.51985085, 0.48014915])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.50095793, 0.49904207]), 'agent_1': array([0.51385355, 0.48614645])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.501848, 0.498152]), 'agent_1': array([0.50758609, 0.49241391])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.50052816, 0.49947184]), 'agent_1': array([0.50395452, 0.49604548])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.50169647, 0.49830353]), 'agent_1': array([0.50117657, 0.49882343])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.50409115, 0.49590885]), 'agent_1': array([0.50055136, 0.49944864])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.50585599, 0.49414401]), 'agent_1': array([0.5001176, 0.4998824])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.50635956, 0.49364044]), 'agent_1': array([0.49888268, 0.50111732])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.50673589, 0.49326411]), 'agent_1': array([0.49761062, 0.50238938])}\n",
      "Final policy:  {'agent_0': array([0.50673589, 0.49326411]), 'agent_1': array([0.49761062, 0.50238938])}\n",
      "\n",
      "Completed Evaluation of Matching Pennies game\n"
     ]
    }
   ],
   "source": [
    "from games.mp import MP\n",
    "\n",
    "EvaluateAll('Matching Pennies', MP(), iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este juego es bastante simple. Entrenando con 2000 iteraciones se observa que no hay un equilibrio. Sin embargo tanto agente como oponente terminan aprendiendo una política equivalente a la aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rock Paper Scisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Evaluation of Rock Paper Scisors game\n",
      "\n",
      "Evaluating agents FP vs RA in Rock Paper Scisors game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.71162791, 0.02790698, 0.26046512]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.81686747, 0.03614458, 0.14698795]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.63739837, 0.26341463, 0.09918699]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.55828221, 0.36687117, 0.07484663]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.58128079, 0.35862069, 0.06009852]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.54979424, 0.4       , 0.05020576]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.51590106, 0.4409894 , 0.04310954]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.45201238, 0.51021672, 0.0377709 ]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.40220386, 0.56418733, 0.03360882]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.37270471, 0.59702233, 0.03027295]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Final policy:  {'agent_0': array([0.37270471, 0.59702233, 0.03027295]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "\n",
      "\n",
      "Evaluating agents FP vs FP in Rock Paper Scisors game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.30733945, 0.36238532, 0.33027523]), 'agent_1': array([0.375     , 0.32211538, 0.30288462])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.3492823 , 0.32057416, 0.33014354]), 'agent_1': array([0.29411765, 0.38235294, 0.32352941])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.32200647, 0.3236246 , 0.35436893]), 'agent_1': array([0.34703947, 0.34375   , 0.30921053])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.32518337, 0.32885086, 0.34596577]), 'agent_1': array([0.3539604 , 0.33663366, 0.30940594])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.33398821, 0.31532417, 0.35068762]), 'agent_1': array([0.33630952, 0.34821429, 0.31547619])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.34318555, 0.32512315, 0.3316913 ]), 'agent_1': array([0.30877483, 0.36009934, 0.33112583])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.34626234, 0.33921016, 0.3145275 ]), 'agent_1': array([0.32315341, 0.32670455, 0.35014205])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.31520396, 0.34981459, 0.33498146]), 'agent_1': array([0.34514925, 0.32587065, 0.3289801 ])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.33883388, 0.31738174, 0.34378438]), 'agent_1': array([0.32522124, 0.34734513, 0.32743363])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.33994054, 0.34093162, 0.31912785]), 'agent_1': array([0.33067729, 0.32171315, 0.34760956])}\n",
      "Final policy:  {'agent_0': array([0.33994054, 0.34093162, 0.31912785]), 'agent_1': array([0.33067729, 0.32171315, 0.34760956])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RA in Rock Paper Scisors game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.29851753, 0.39659099, 0.30489148]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.30709378, 0.37224982, 0.32065641]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.31232018, 0.36314934, 0.32453049]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.31408962, 0.35835661, 0.32755377]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.31610532, 0.35537369, 0.32852099]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.31828885, 0.35305115, 0.32866   ]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.31983912, 0.35086739, 0.32929349]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.32087784, 0.34927403, 0.32984814]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.32151146, 0.34797472, 0.33051382]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.32205676, 0.3471782 , 0.33076504]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "Final policy:  {'agent_0': array([0.32205676, 0.3471782 , 0.33076504]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs FP in Rock Paper Scisors game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.2670817 , 0.36400879, 0.36890951]), 'agent_1': array([0.32258065, 0.33640553, 0.34101382])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.33726512, 0.3096353 , 0.35309958]), 'agent_1': array([0.35251799, 0.28057554, 0.36690647])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.32537681, 0.32541913, 0.34920406]), 'agent_1': array([0.31928687, 0.43273906, 0.24797407])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.3066818 , 0.34312014, 0.35019806]), 'agent_1': array([0.46756426, 0.34394125, 0.18849449])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.30886797, 0.3511314 , 0.34000063]), 'agent_1': array([0.37758112, 0.27630285, 0.34611603])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.32314531, 0.34505731, 0.33179738]), 'agent_1': array([0.31552999, 0.23089565, 0.45357436])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.33255229, 0.33698687, 0.33046084]), 'agent_1': array([0.27170078, 0.33027523, 0.39802399])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.33202665, 0.33072741, 0.33724594]), 'agent_1': array([0.23809524, 0.4131107 , 0.34879406])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.32890894, 0.32841421, 0.34267685]), 'agent_1': array([0.31095212, 0.37864612, 0.31040176])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.32595748, 0.3313216 , 0.34272092]), 'agent_1': array([0.37927615, 0.34110064, 0.2796232 ])}\n",
      "Final policy:  {'agent_0': array([0.32595748, 0.3313216 , 0.34272092]), 'agent_1': array([0.37927615, 0.34110064, 0.2796232 ])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RM in Rock Paper Scisors game for 2000 iterations\n",
      "Progress: 200/2000 -  {'agent_0': array([0.35560866, 0.30687506, 0.33751627]), 'agent_1': array([0.34356529, 0.33335399, 0.32308072])}\n",
      "Progress: 400/2000 -  {'agent_0': array([0.34532304, 0.31700726, 0.3376697 ]), 'agent_1': array([0.34574704, 0.32986052, 0.32439244])}\n",
      "Progress: 600/2000 -  {'agent_0': array([0.33714054, 0.32736199, 0.33549746]), 'agent_1': array([0.34111705, 0.33606857, 0.32281437])}\n",
      "Progress: 800/2000 -  {'agent_0': array([0.33341809, 0.33222132, 0.33436059]), 'agent_1': array([0.33852028, 0.33775141, 0.32372831])}\n",
      "Progress: 1000/2000 -  {'agent_0': array([0.33104656, 0.33480608, 0.33414736]), 'agent_1': array([0.33735707, 0.33634969, 0.32629323])}\n",
      "Progress: 1200/2000 -  {'agent_0': array([0.32986769, 0.33626964, 0.33386267]), 'agent_1': array([0.33739374, 0.33484766, 0.3277586 ])}\n",
      "Progress: 1400/2000 -  {'agent_0': array([0.32885494, 0.33765165, 0.33349341]), 'agent_1': array([0.33712534, 0.33367728, 0.32919738])}\n",
      "Progress: 1600/2000 -  {'agent_0': array([0.3279455 , 0.33915484, 0.33289967]), 'agent_1': array([0.33606575, 0.33339239, 0.33054185])}\n",
      "Progress: 1800/2000 -  {'agent_0': array([0.32721623, 0.33988541, 0.33289836]), 'agent_1': array([0.33562521, 0.33332049, 0.3310543 ])}\n",
      "Progress: 2000/2000 -  {'agent_0': array([0.32657761, 0.34008556, 0.33333683]), 'agent_1': array([0.33552485, 0.33307539, 0.33139976])}\n",
      "Final policy:  {'agent_0': array([0.32657761, 0.34008556, 0.33333683]), 'agent_1': array([0.33552485, 0.33307539, 0.33139976])}\n",
      "\n",
      "Completed Evaluation of Rock Paper Scisors game\n"
     ]
    }
   ],
   "source": [
    "from games.rps import RPS\n",
    "\n",
    "EvaluateAll('Rock Paper Scisors', RPS(), iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en matching pennies, el juego es bastante simple. Al entrenarlo con 2000 itraciones se puede observar que tampoco se llega a un equilibrio. La política aprendida por los ageentes también converge a la política aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blotto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Evaluation of Blotto 6 game\n",
      "\n",
      "Evaluating agents FP vs RA in Blotto 6 game for 10000 iterations\n",
      "Progress: 1000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 2000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 3000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 4000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 5000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 6000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 7000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 8000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 9000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 10000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Final policy:  {'agent_0': array([7.98164222e-04, 6.98393695e-04, 9.98503442e-01]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "\n",
      "\n",
      "Evaluating agents FP vs FP in Blotto 6 game for 10000 iterations\n",
      "Progress: 1000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 2000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 3000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 4000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 5000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 6000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 7000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 8000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 9000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 10000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Final policy:  {'agent_0': array([1.99880072e-04, 1.99880072e-04, 9.99600240e-01]), 'agent_1': array([6.98812020e-04, 7.98642308e-04, 9.98502546e-01])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RA in Blotto 6 game for 10000 iterations\n",
      "Progress: 1000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 2000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 3000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 4000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 5000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 6000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 7000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 8000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 9000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Progress: 10000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [1, 1, 4]}\n",
      "Final policy:  {'agent_0': array([3.33333333e-04, 2.26164012e-02, 9.77050265e-01]), 'agent_1': array([0.33333333, 0.33333333, 0.33333333])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs FP in Blotto 6 game for 10000 iterations\n",
      "Progress: 1000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 2000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 3000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 4000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 5000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 6000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 7000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 8000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 9000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Progress: 10000/10000 -  {'agent_0': [1, 2, 3], 'agent_1': [2, 2, 2]}\n",
      "Final policy:  {'agent_0': array([6.66666667e-05, 4.99966667e-01, 4.99966667e-01]), 'agent_1': array([2.99670363e-04, 3.99560483e-04, 9.99300769e-01])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RM in Blotto 6 game for 10000 iterations\n",
      "Progress: 1000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 2000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 3000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 4000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 5000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 6000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 7000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 8000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 9000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Progress: 10000/10000 -  {'agent_0': [2, 2, 2], 'agent_1': [2, 2, 2]}\n",
      "Final policy:  {'agent_0': array([6.66666667e-05, 6.66666667e-05, 9.99866667e-01]), 'agent_1': array([6.66666667e-05, 6.66666667e-05, 9.99866667e-01])}\n",
      "\n",
      "Completed Evaluation of Blotto 6 game\n"
     ]
    }
   ],
   "source": [
    "from games.blotto import Blotto\n",
    "\n",
    "EvaluateAll('Blotto 6', Blotto(S=6, N=3), lambda game, agent: game._moves[np.argmax(agent.policy())], iterations=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este juego es un poco más interesante. En el progreso se muestra la jugada de mayor probabilidad según la política para mantener el formato de la salida simple.\n",
    "\n",
    "Se dejó entrenar por 10000 episodios. Se observa que cuando juegan 2 agentes FP, se alcanza un equilibrio jugando `[2,2,2]` mientras que los agentes RM encuentran como jugadas óptimas `[1,2,3]` y `[2,2,2]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blotto 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Evaluation of Blotto 12 game\n",
      "\n",
      "Evaluating agents FP vs RA in Blotto 12 game for 20000 iterations\n",
      "Progress: 2000/20000 -  {'agent_0': [3, 4, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 4000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 6000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 8000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 10000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 12000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 14000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 16000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 18000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Progress: 20000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [1, 1, 10]}\n",
      "Final policy:  {'agent_0': array([3.49040140e-04, 2.49314385e-04, 1.99451508e-04, 1.49588631e-04,\n",
      "       1.49588631e-04, 4.98628771e-05, 9.97257542e-05, 3.98903017e-04,\n",
      "       7.84941411e-01, 2.49314385e-04, 7.52430815e-02, 1.37920718e-01]), 'agent_1': array([0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333,\n",
      "       0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333,\n",
      "       0.08333333, 0.08333333])}\n",
      "\n",
      "\n",
      "Evaluating agents FP vs FP in Blotto 12 game for 20000 iterations\n",
      "Progress: 2000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [2, 3, 7]}\n",
      "Progress: 4000/20000 -  {'agent_0': [2, 3, 7], 'agent_1': [1, 5, 6]}\n",
      "Progress: 6000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [1, 5, 6]}\n",
      "Progress: 8000/20000 -  {'agent_0': [1, 5, 6], 'agent_1': [1, 5, 6]}\n",
      "Progress: 10000/20000 -  {'agent_0': [1, 5, 6], 'agent_1': [3, 3, 6]}\n",
      "Progress: 12000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [3, 3, 6]}\n",
      "Progress: 14000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [3, 3, 6]}\n",
      "Progress: 16000/20000 -  {'agent_0': [2, 5, 5], 'agent_1': [2, 5, 5]}\n",
      "Progress: 18000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 20000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Final policy:  {'agent_0': array([1.49611011e-04, 4.98703371e-05, 9.97406742e-05, 8.24855376e-02,\n",
      "       1.75543587e-01, 2.49351686e-04, 1.72850588e-01, 2.49351686e-04,\n",
      "       1.94344704e-01, 1.95142629e-01, 7.33592659e-02, 1.05475763e-01]), 'agent_1': array([1.49633398e-04, 1.99511198e-04, 4.98777994e-05, 8.19492244e-02,\n",
      "       1.87091626e-01, 2.49388997e-04, 1.70532196e-01, 5.48655793e-04,\n",
      "       1.94772807e-01, 1.86493092e-01, 7.24724425e-02, 1.05491546e-01])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RA in Blotto 12 game for 20000 iterations\n",
      "Progress: 2000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 4000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 6000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 8000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 10000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 12000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 14000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 16000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 18000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Progress: 20000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [1, 1, 10]}\n",
      "Final policy:  {'agent_0': array([0.01175399, 0.01721247, 0.02922125, 0.06879544, 0.09986211,\n",
      "       0.08902916, 0.08286927, 0.09168436, 0.13150278, 0.10722431,\n",
      "       0.12716687, 0.14367799]), 'agent_1': array([0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333,\n",
      "       0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333,\n",
      "       0.08333333, 0.08333333])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs FP in Blotto 12 game for 20000 iterations\n",
      "Progress: 2000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 4000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [4, 4, 4]}\n",
      "Progress: 6000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 8000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 10000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 12000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 14000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 16000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 18000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Progress: 20000/20000 -  {'agent_0': [3, 3, 6], 'agent_1': [2, 5, 5]}\n",
      "Final policy:  {'agent_0': array([0.00467051, 0.00467051, 0.0172403 , 0.0953048 , 0.15648957,\n",
      "       0.06180516, 0.07406331, 0.08565043, 0.14433225, 0.18107313,\n",
      "       0.08252282, 0.09217719]), 'agent_1': array([3.98624745e-04, 4.48452838e-04, 4.98280931e-05, 4.48452838e-04,\n",
      "       1.40515222e-01, 1.49484279e-04, 7.47421396e-04, 4.83332503e-03,\n",
      "       3.56071553e-01, 1.67372565e-01, 4.48452838e-04, 3.28516618e-01])}\n",
      "\n",
      "\n",
      "Evaluating agents RM vs RM in Blotto 12 game for 20000 iterations\n",
      "Progress: 2000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 4000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 6000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 8000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 10000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 12000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 14000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 16000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 18000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Progress: 20000/20000 -  {'agent_0': [4, 4, 4], 'agent_1': [4, 4, 4]}\n",
      "Final policy:  {'agent_0': array([0.01238907, 0.01879885, 0.03187374, 0.08204342, 0.12542835,\n",
      "       0.09089623, 0.08156794, 0.0789121 , 0.12413643, 0.11198841,\n",
      "       0.11143055, 0.13053491]), 'agent_1': array([0.0120287 , 0.01859602, 0.0323488 , 0.08085492, 0.12275984,\n",
      "       0.0942448 , 0.08417035, 0.0789906 , 0.12257735, 0.11392863,\n",
      "       0.110876  , 0.12862398])}\n",
      "\n",
      "Completed Evaluation of Blotto 12 game\n"
     ]
    }
   ],
   "source": [
    "EvaluateAll('Blotto 12', Blotto(S=12, N=3), lambda game, agent: game._moves[np.argmax(agent.policy())], iterations=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta ejecución se dejó entrenar por 20000 iteraciones, para permitir que tanto FP como RM converjan. Las distribuciones de probabilidades finales muestran diferencias marginales entre algunas acciones disponibles, por lo que es probable que dejando entrenar un número mayor de iteraciones haya algún movimiento hacia otra posición."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
